{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sec_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reckoning-machines/oil_betas/blob/master/sec_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxREunso0REA",
        "colab_type": "code",
        "outputId": "663aa94c-5785-471f-e1e8-5fcb1e24dfe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#i've had best success with Runtime / Change Runtime Type / Hardware Accelerator = None\n",
        "\n",
        "# why use R here?  \n",
        "# edgarWebR pulls sections really well\n",
        "!rm -r 'sec_test'\n",
        "!rm -r 'Archives'\n",
        "!git clone https://github.com/reckoning-machines/sec_test.git #errors if exists but doesn't halt exe\n",
        "!cp \"sec_test/test_ticker_list.csv\" \"test_ticker_list.csv\" #move ticker list to root\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'sec_test'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/37)\u001b[K\rremote: Counting objects:   5% (2/37)\u001b[K\rremote: Counting objects:   8% (3/37)\u001b[K\rremote: Counting objects:  10% (4/37)\u001b[K\rremote: Counting objects:  13% (5/37)\u001b[K\rremote: Counting objects:  16% (6/37)\u001b[K\rremote: Counting objects:  18% (7/37)\u001b[K\rremote: Counting objects:  21% (8/37)\u001b[K\rremote: Counting objects:  24% (9/37)\u001b[K\rremote: Counting objects:  27% (10/37)\u001b[K\rremote: Counting objects:  29% (11/37)\u001b[K\rremote: Counting objects:  32% (12/37)\u001b[K\rremote: Counting objects:  35% (13/37)\u001b[K\rremote: Counting objects:  37% (14/37)\u001b[K\rremote: Counting objects:  40% (15/37)\u001b[K\rremote: Counting objects:  43% (16/37)\u001b[K\rremote: Counting objects:  45% (17/37)\u001b[K\rremote: Counting objects:  48% (18/37)\u001b[K\rremote: Counting objects:  51% (19/37)\u001b[K\rremote: Counting objects:  54% (20/37)\u001b[K\rremote: Counting objects:  56% (21/37)\u001b[K\rremote: Counting objects:  59% (22/37)\u001b[K\rremote: Counting objects:  62% (23/37)\u001b[K\rremote: Counting objects:  64% (24/37)\u001b[K\rremote: Counting objects:  67% (25/37)\u001b[K\rremote: Counting objects:  70% (26/37)\u001b[K\rremote: Counting objects:  72% (27/37)\u001b[K\rremote: Counting objects:  75% (28/37)\u001b[K\rremote: Counting objects:  78% (29/37)\u001b[K\rremote: Counting objects:  81% (30/37)\u001b[K\rremote: Counting objects:  83% (31/37)\u001b[K\rremote: Counting objects:  86% (32/37)\u001b[K\rremote: Counting objects:  89% (33/37)\u001b[K\rremote: Counting objects:  91% (34/37)\u001b[K\rremote: Counting objects:  94% (35/37)\u001b[K\rremote: Counting objects:  97% (36/37)\u001b[K\rremote: Counting objects: 100% (37/37)\u001b[K\rremote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects:   2% (1/36)\u001b[K\rremote: Compressing objects:   5% (2/36)\u001b[K\rremote: Compressing objects:   8% (3/36)\u001b[K\rremote: Compressing objects:  11% (4/36)\u001b[K\rremote: Compressing objects:  13% (5/36)\u001b[K\rremote: Compressing objects:  16% (6/36)\u001b[K\rremote: Compressing objects:  19% (7/36)\u001b[K\rremote: Compressing objects:  22% (8/36)\u001b[K\rremote: Compressing objects:  25% (9/36)\u001b[K\rremote: Compressing objects:  27% (10/36)\u001b[K\rremote: Compressing objects:  30% (11/36)\u001b[K\rremote: Compressing objects:  33% (12/36)\u001b[K\rremote: Compressing objects:  36% (13/36)\u001b[K\rremote: Compressing objects:  38% (14/36)\u001b[K\rremote: Compressing objects:  41% (15/36)\u001b[K\rremote: Compressing objects:  44% (16/36)\u001b[K\rremote: Compressing objects:  47% (17/36)\u001b[K\rremote: Compressing objects:  50% (18/36)\u001b[K\rremote: Compressing objects:  52% (19/36)\u001b[K\rremote: Compressing objects:  55% (20/36)\u001b[K\rremote: Compressing objects:  58% (21/36)\u001b[K\rremote: Compressing objects:  61% (22/36)\u001b[K\rremote: Compressing objects:  63% (23/36)\u001b[K\rremote: Compressing objects:  66% (24/36)\u001b[K\rremote: Compressing objects:  69% (25/36)\u001b[K\rremote: Compressing objects:  72% (26/36)\u001b[K\rremote: Compressing objects:  75% (27/36)\u001b[K\rremote: Compressing objects:  77% (28/36)\u001b[K\rremote: Compressing objects:  80% (29/36)\u001b[K\rremote: Compressing objects:  83% (30/36)\u001b[K\rremote: Compressing objects:  86% (31/36)\u001b[K\rremote: Compressing objects:  88% (32/36)\u001b[K\rremote: Compressing objects:  91% (33/36)\u001b[K\rremote: Compressing objects:  94% (34/36)\u001b[K\rremote: Compressing objects:  97% (35/36)\u001b[K\rremote: Compressing objects: 100% (36/36)\u001b[K\rremote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "Unpacking objects:   2% (1/37)   \rUnpacking objects:   5% (2/37)   \rUnpacking objects:   8% (3/37)   \rUnpacking objects:  10% (4/37)   \rUnpacking objects:  13% (5/37)   \rUnpacking objects:  16% (6/37)   \rUnpacking objects:  18% (7/37)   \rUnpacking objects:  21% (8/37)   \rUnpacking objects:  24% (9/37)   \rUnpacking objects:  27% (10/37)   \rUnpacking objects:  29% (11/37)   \rUnpacking objects:  32% (12/37)   \rUnpacking objects:  35% (13/37)   \rUnpacking objects:  37% (14/37)   \rUnpacking objects:  40% (15/37)   \rremote: Total 37 (delta 22), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects:  43% (16/37)   \rUnpacking objects:  45% (17/37)   \rUnpacking objects:  48% (18/37)   \rUnpacking objects:  51% (19/37)   \rUnpacking objects:  54% (20/37)   \rUnpacking objects:  56% (21/37)   \rUnpacking objects:  59% (22/37)   \rUnpacking objects:  62% (23/37)   \rUnpacking objects:  64% (24/37)   \rUnpacking objects:  67% (25/37)   \rUnpacking objects:  70% (26/37)   \rUnpacking objects:  72% (27/37)   \rUnpacking objects:  75% (28/37)   \rUnpacking objects:  78% (29/37)   \rUnpacking objects:  81% (30/37)   \rUnpacking objects:  83% (31/37)   \rUnpacking objects:  86% (32/37)   \rUnpacking objects:  89% (33/37)   \rUnpacking objects:  91% (34/37)   \rUnpacking objects:  94% (35/37)   \rUnpacking objects:  97% (36/37)   \rUnpacking objects: 100% (37/37)   \rUnpacking objects: 100% (37/37), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04jdag6BhSmY",
        "colab_type": "code",
        "outputId": "2f462a40-5ca7-4f68-e9d1-eb89716d9e31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# activate R magic\n",
        "import rpy2\n",
        "%load_ext rpy2.ipython"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/rpy2/robjects/pandas2ri.py:14: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.\n",
            "  from pandas.core.index import Index as PandasIndex\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/robjects/pandas2ri.py:34: UserWarning: pandas >= 1.0 is not supported.\n",
            "  warnings.warn('pandas >= 1.0 is not supported.')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSYJWc0DyRFR",
        "colab_type": "code",
        "outputId": "dc83e1f1-9a54-4120-f382-58badbf86a09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "%%R\n",
        "#installs take a bit of time...\n",
        "print('Startup: install packages... (may take a few minutes the first time)')\n",
        "\n",
        "library(devtools)\n",
        "\n",
        "devtools::install_github(\"mwaldstein/edgarWebR\",quietly=TRUE)\n",
        "print('Done with edgarWebR install')\n",
        "devtools::install_github(\"r-lib/xml2\",quietly=TRUE)\n",
        "print('Done with xml2 install')\n",
        "#devtools::install_github(\"DavisVaughan/furrr\")\n",
        "install.packages('furrr',quiet = TRUE)\n",
        "print('Done with furr install')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1] \"Startup: install packages... (may take a few minutes the first time)\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "R[write to console]: Skipping install of 'edgarWebR' from a github remote, the SHA1 (e7fa70ea) has not changed since last install.\n",
            "  Use `force = TRUE` to force installation\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1] \"Done with edgarWebR install\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "R[write to console]: Skipping install of 'xml2' from a github remote, the SHA1 (876759f3) has not changed since last install.\n",
            "  Use `force = TRUE` to force installation\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1] \"Done with xml2 install\"\n",
            "[1] \"Done with furr install\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2x4-h6m0x-V",
        "colab_type": "code",
        "outputId": "835f90c8-90bd-48c8-b5af-4dc767ab2f3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "%%R\n",
        "\n",
        "#this run also can take a bit of time...\n",
        "\n",
        "library(edgarWebR) #this is an up to date library with an active maintainer.\n",
        "library(xml2)\n",
        "library(knitr)\n",
        "library(dplyr)\n",
        "library(purrr)\n",
        "library(rvest)\n",
        "library(tidyr)\n",
        "library(readr)\n",
        "#library(textclean) #not using this yet.\n",
        "library(furrr) #multiprocessing... does colab use it\n",
        "\n",
        "LOGFILE = format(Sys.time(), \"%b_%d_%Y.log\")\n",
        "print(LOGFILE)\n",
        "\n",
        "CSVFILE = format(Sys.time(), \"%b_%d_%Y.csv\")\n",
        "print(CSVFILE)\n",
        "\n",
        "get_filings_links <-function(str_ticker) {\n",
        "    df_filings <- company_filings(str_ticker, type = \"10-\", count = 20)\n",
        "    df_filings <- df_filings[df_filings$type == \"10-K\" | df_filings$type == \"10-Q\", ]\n",
        "    df_filing_infos <- map_df(df_filings$href, filing_information)\n",
        "    df_filings <- bind_cols(df_filings, df_filing_infos)\n",
        "    return(head(as_tibble(df_filings),20))\n",
        "  }\n",
        "\n",
        "write_log <- function(str_text) {\n",
        "      print(str_text)\n",
        "      if (file.exists(LOGFILE)) {\n",
        "          write(str_text,file=LOGFILE,append=TRUE)\n",
        "      } else {\n",
        "          write(str_text,file=LOGFILE,append=FALSE)\n",
        "      }\n",
        "\n",
        "  }\n",
        "\n",
        "write_log_csv <- function(df) {\n",
        "    if (file.exists(CSVFILE)) {\n",
        "          write_csv(df,CSVFILE,append=TRUE)\n",
        "      } else {\n",
        "          write_csv(df,CSVFILE,append=FALSE)\n",
        "      }\n",
        "\n",
        "  }\n",
        "\n",
        "get_mdna_text <- function(str_href) {\n",
        "  write_log(\"next link:\")\n",
        "  write_log(str_href)\n",
        "\n",
        "  #make this a func\n",
        "  str_file_path <- ''\n",
        "  file_path = strsplit(str_href,'/')\n",
        "  for (i in 5:length(file_path[[1]])-1) {\n",
        "    str_file_path = paste0(str_file_path,\"/\",(file_path[[1]][i]))\n",
        "  }\n",
        "  str_file_path <- paste0(getwd(),\"/\",str_file_path)\n",
        "  dir.create(str_file_path,recursive = TRUE)\n",
        "  str_file_path\n",
        "  str_file_name <- ''\n",
        "  file_path = strsplit(str_href,'/')\n",
        "  for (i in 4:length(file_path[[1]])) {\n",
        "    str_file_name = paste0(str_file_name,\"/\",(file_path[[1]][i]))\n",
        "  }\n",
        "  str_file_name <- paste0(getwd(),str_file_name)\n",
        "  str_file_name <- gsub(\".htm\",\".csv\",str_file_name)\n",
        "  \n",
        "  str_section = 'item 2|item 7'\n",
        "  str_search = 'discussion'\n",
        "\n",
        "  if (file.exists(str_file_name)) {  #add force equals true\n",
        "    write_log(\"filing documents from cache ...\")\n",
        "    \n",
        "    df_filing_documents <- read_csv(str_file_name,col_types = cols()) \n",
        "    df_filing_documents <- df_filing_documents %>% mutate_if(is.logical, as.character)\n",
        "  } else {\n",
        "    write_log(\"filing documents from sec ...\")\n",
        "    \n",
        "    df_filing_documents <- filing_documents(str_href) %>%\n",
        "      filter(!grepl('.pdf',href)) %>%\n",
        "      write_csv(str_file_name)\n",
        "  }\n",
        "  \n",
        "  str_doc_href <- df_filing_documents[df_filing_documents$type == \"10-K\" | df_filing_documents$type == \"10-Q\",]$href\n",
        "  \n",
        "  print(df_filing_documents[df_filing_documents$type == \"10-K\" | df_filing_documents$type == \"10-Q\",])  \n",
        "  \n",
        "  file_end <- gsub(\"https://www.sec.gov\",'',str_doc_href)\n",
        "  \n",
        "  file_name = paste0(getwd(),file_end)\n",
        "  \n",
        "  #use cache if possible\n",
        "  if (file.exists(file_name)) {\n",
        "\n",
        "    doc <- read_csv(file_name,col_types = cols(.default = \"c\"))\n",
        "    print(\"local cache\")\n",
        "    \n",
        "  } else {\n",
        "\n",
        "    doc <- parse_filing(str_doc_href)    \n",
        "\n",
        "    str_file_path <- ''\n",
        "    file_path = strsplit(file_name,'/')\n",
        "    for (i in 3:length(file_path[[1]])-1) {\n",
        "      str_file_path = paste0(str_file_path,\"/\",(file_path[[1]][i]))\n",
        "    }\n",
        "    str_file_path <- paste0(str_file_path,\"/\")\n",
        "    dir.create(str_file_path,recursive = TRUE)\n",
        "    write_csv(as_tibble(doc),file_name)\n",
        "    \n",
        "  }\n",
        "\n",
        "  df_txt <- doc[grepl(str_section, doc$item.name, ignore.case = TRUE) & grepl(str_search, doc$item.name, ignore.case = TRUE), ] # only discussion for now\n",
        "  #if default search fails, use a dictionary attempt\n",
        "  if (nrow(df_txt) == 0) {\n",
        "    write_log('going to backup')\n",
        "    #paired vector of start and ending text to slice if found\n",
        "    #going forward use tickers as an additional column\n",
        "    #and port this to a csv file as part of the install.\n",
        "    df_filter_list <- data.frame(\n",
        "      start_text = c('Introduction',\n",
        "                     'FUNCTIONAL EARNINGS', \n",
        "                     'DISCUSSION AND ANALYSIS',\n",
        "                     'DISCUSSION AND ANALYSIS',\n",
        "                     'DISCUSSION AND ANALYSIS',\n",
        "                     'OVERVIEW',\n",
        "                     'Business Overview',\n",
        "                     'Financial Review',\n",
        "                     'RESULTS OF OPERATIONS',\n",
        "                     'Overview',\n",
        "                     'Entergy operates',\n",
        "                     \"MANAGEMENT\\'S FINANCIAL DISCUSSION\",\n",
        "                     'General',\n",
        "                     \"Management's Discussion\",\n",
        "                     'EXECUTIVE SUMMARY',\n",
        "                     'EXECUTIVE OVERVIEW',\n",
        "                     'EXECUTIVE OVERVIEW',\n",
        "                     'The following management discussion and analysis',\n",
        "                     'CURRENT ECONOMIC CONDITIONS',\n",
        "                     'Overview and Highlights',\n",
        "                     'Financial Review - Results of Operations'),\n",
        "      end_text = c('Quantitative and qualitative disclosures about market risk',\n",
        "                   \"MANAGEMENT\\'S REPORT\",\n",
        "                   'RISK FACTORS',\n",
        "                   'FIVE-YEAR PERFORMANCE GRAPH',\n",
        "                   'FINANCIAL STATEMENTS AND NOTES',\n",
        "                   'Risk management includes the identification',\n",
        "                   'Selected Loan Maturity Data',\n",
        "                   'Risk Management',\n",
        "                   'QUANTITATIVE AND QUALITATIVE DISCLOSURES',\n",
        "                   'Forward-Looking Statements',\n",
        "                   'New Accounting Pronouncements',\n",
        "                   'New Accounting Pronouncements',\n",
        "                   'Website information',\n",
        "                   'Risk Disclosures',\n",
        "                   'RISK FACTORS',\n",
        "                   'A summary of contractual obligations is included',\n",
        "                   'CONSOLIDATED RESULTS OF OPERATIONS',\n",
        "                   'NON-GAAP FINANCIAL MEASURES',\n",
        "                   'FORWARD-LOOKING STATEMENTS',\n",
        "                   'Critical Accounting Policies and Estimates',\n",
        "                   'Unregistered Sales of Equity Securities and Use of Proceeds')\n",
        "    )\n",
        "    \n",
        "    #this would be case sensitive\n",
        "    for (row in 1:nrow(df_filter_list)) { #should flip this to apply()\n",
        "\n",
        "      start_text <- df_filter_list[row, \"start_text\"]\n",
        "      end_text <- df_filter_list[row, \"end_text\"]\n",
        "\n",
        "      write_log(paste0('trying ',start_text))\n",
        "      write_log(paste0('to ',end_text))\n",
        "\n",
        "      i_start = as.integer(which(grepl(start_text, doc$text))) \n",
        "      if (length(i_start) > 1) { #handle table of contents duplicates\n",
        "        i_start = i_start[2]\n",
        "      }\n",
        "      i_end = as.integer(which(grepl(end_text, doc$text)))\n",
        "      if (length(i_end) > 1) {\n",
        "        i_end = i_end[2]\n",
        "      }\n",
        "\n",
        "      write_log(i_start)\n",
        "      write_log(i_end)\n",
        "\n",
        "      if (length(i_start) != 0 & length(i_end) != 0) {\n",
        "        #i_start = as.numeric(i_start)\n",
        "        #i_end = as.numeric(i_end)\n",
        "        if (i_start < i_end) {        \n",
        "            print(paste0('istart is:',i_start,' iend is:',i_end))\n",
        "            df_txt = doc[i_start:i_end,]\n",
        "            break\n",
        "        }\n",
        "      }\n",
        "\n",
        "    }\n",
        "    if (length(i_start) == 0 || length(i_end) == 0) {\n",
        "      write_log(\"missing section for:\")\n",
        "      write_log(str_href)\n",
        "    }\n",
        "\n",
        "  }\n",
        "  #we could do some text preprocessing here.\n",
        "\n",
        "  df_txt <- as_tibble(df_txt) %>%\n",
        "    #mutate(text = textclean::strip(text)) %>%\n",
        "    mutate(section = str_search)\n",
        "\n",
        "  return(df_txt)\n",
        "}\n",
        "\n",
        "get_document_text <- function(str_ticker, force = FALSE) { #not using force yet\n",
        "  start_time <- Sys.time()\n",
        "\n",
        "  write_log(str_ticker)\n",
        "\n",
        "  str_write_name <- paste0('sec_data_folder/',str_ticker)\n",
        "\n",
        "  write_log(\"get filings links ...\")\n",
        "\n",
        "  filings_csv <- paste0(str_write_name,\"_filings.csv\")\n",
        "  \n",
        "  if (file.exists(filings_csv)) {  #add force equals true\n",
        "    write_log(\"from cache ...\")\n",
        "    \n",
        "    df_filings <- read_csv(filings_csv,col_types = cols()) \n",
        "    df_filings <- df_filings %>% mutate_if(is.logical, as.character)\n",
        "    } else {\n",
        "    write_log(\"from sec ...\")\n",
        "      \n",
        "    df_filings <- get_filings_links(str_ticker) %>%\n",
        "      mutate(ticker = str_ticker) %>%\n",
        "      write_csv(filings_csv)\n",
        "    }\n",
        "\n",
        "  write_log_csv(df_filings)\n",
        "  \n",
        "#for debug\n",
        "  i_test = nrow(df_filings) #for some reason this won't evaulate inside the if statement\n",
        "  if (i_test == 0) {\n",
        "      return(NULL)\n",
        "  }\n",
        "\n",
        "  write_log(\"get section text ...\")\n",
        "\n",
        "  df_data <- (df_filings) %>%\n",
        "    rowwise() %>%\n",
        "    mutate(nest_discussion = map(.x = href, .f = get_mdna_text)) %>%\n",
        "    ungroup() %>%\n",
        "    group_by(period_date) %>%\n",
        "    arrange(desc(period_date))\n",
        "\n",
        "  #jenky - find a rowwise application\n",
        "  a <- df_data %>%\n",
        "    select(period_date,filing_date,type,form_name,documents,nest_discussion) %>%\n",
        "    unnest(nest_discussion)\n",
        "\n",
        "  write_log(\"write to local csv  ...\")\n",
        "  df_data <- a %>%\n",
        "    as_tibble() %>%\n",
        "    write_csv(paste0(str_write_name,\".csv\"))\n",
        "\n",
        "  end_time <- Sys.time()\n",
        "\n",
        "  write_log(end_time - start_time)\n",
        "\n",
        "  return(df_data)\n",
        "}\n",
        "\n",
        "df_tickers <- read_csv('test_ticker_list.csv',col_types = cols()) \n",
        "\n",
        "dir.create('sec_data_folder', showWarnings = FALSE)\n",
        "\n",
        "future::plan(multiprocess)\n",
        "\n",
        "df_data <- future_map_dfr(df_tickers$Symbol, get_document_text,.progress = TRUE) #takes a few minutes\n",
        "#df_data <- map_df(df_tickers$Symbol, get_document_text) # non parallel version\n",
        "print('done')\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R[write to console]: \n",
            "Attaching package: ‘dplyr’\n",
            "\n",
            "\n",
            "R[write to console]: The following objects are masked from ‘package:stats’:\n",
            "\n",
            "    filter, lag\n",
            "\n",
            "\n",
            "R[write to console]: The following objects are masked from ‘package:base’:\n",
            "\n",
            "    intersect, setdiff, setequal, union\n",
            "\n",
            "\n",
            "R[write to console]: \n",
            "Attaching package: ‘rvest’\n",
            "\n",
            "\n",
            "R[write to console]: The following object is masked from ‘package:purrr’:\n",
            "\n",
            "    pluck\n",
            "\n",
            "\n",
            "R[write to console]: \n",
            "Attaching package: ‘readr’\n",
            "\n",
            "\n",
            "R[write to console]: The following object is masked from ‘package:rvest’:\n",
            "\n",
            "    guess_encoding\n",
            "\n",
            "\n",
            "R[write to console]: Loading required package: future\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1] \"May_27_2020.log\"\n",
            "[1] \"May_27_2020.csv\"\n",
            " Progress:                                                                  100%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "R[write to console]: \n",
            "\n",
            "R[write to console]: Warning message:\n",
            "\n",
            "R[write to console]: Missing column names filled in: 'X1' [1] \n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Error in withCallingHandlers(expr, packageStartupMessage = function(c) invokeRestart(\"muffleMessage\")) : \n",
            "  argument \"expr\" is missing, with no default\n",
            "Calls: <Anonymous> ... suppressPackageStartupMessages -> withCallingHandlers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UYcX52F7_pu",
        "colab_type": "code",
        "outputId": "c7aab7d5-ce1d-4070-9b24-ba993457a04f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "#move from R to python\n",
        "\n",
        "!pip install pandarallel\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "#from transformers import pipeline\n",
        "#nlp = pipeline('sentiment-analysis')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk import tokenize\n",
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None\n",
        "import os\n",
        "import pandas as pd\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "import numpy as np\n",
        "from pandarallel import pandarallel\n",
        "\n",
        "import time\n",
        "\n",
        "pandarallel.initialize()\n",
        "df_tickers = pd.read_csv('test_ticker_list.csv')\n",
        "\n",
        "LOGFILE = 'sec_nlp_beta.log'\n",
        "f = open(LOGFILE, \"w\")\n",
        "t = time.localtime()\n",
        "current_time = time.strftime(\"%H:%M:%S\", t)\n",
        "f.write(current_time+\": process started\")\n",
        "f.close()\n",
        "\n",
        "FIND_WORDS = ['covid',\n",
        "              'guidance',\n",
        "              'outlook']\n",
        "\n",
        "def check_if_list_found_in_text(text, words=[], return_offset=False, lower_text=True):\n",
        "    result = []\n",
        "    text = (\n",
        "        \" \"\n",
        "        + text.replace(\"_\", \" \")\n",
        "        .replace(\"-\", \" \")\n",
        "        .replace(\",\", \" \")\n",
        "        .replace(\";\", \" \")\n",
        "        .replace('\"', \" \")\n",
        "        .replace(\":\", \" \")\n",
        "        .replace(\".\", \" \")\n",
        "        + \" \"\n",
        "    )\n",
        "    if lower_text:\n",
        "        text = text.lower()\n",
        "    for word in words:\n",
        "        word = (\n",
        "            \" \"\n",
        "            + word.replace(\"_\", \" \")\n",
        "            .replace(\"-\", \" \")\n",
        "            .replace(\",\", \" \")\n",
        "            .replace(\";\", \" \")\n",
        "            .replace('\"', \" \")\n",
        "            .replace(\":\", \" \")\n",
        "            .replace(\".\", \" \")\n",
        "            + \" \"\n",
        "        )\n",
        "        if lower_text:\n",
        "            word = word.lower()\n",
        "        if word in text:\n",
        "            if return_offset:\n",
        "                offset = text.find(word)\n",
        "                # offset = offset if not offset else offset-1\n",
        "                result.append(offset)\n",
        "            else:\n",
        "                result.append(word.strip())\n",
        "    return result\n",
        "\n",
        "def filter_stopwords(sent):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(sent)\n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "    filtered_sentence = []\n",
        "    for w in word_tokens:\n",
        "        if w not in stop_words:\n",
        "            filtered_sentence.append(w)\n",
        "    return ' '.join(filtered_sentence)\n",
        "\n",
        "def sentiment_from_text(sentence):\n",
        "  sentence = filter_stopwords(sentence)\n",
        "  list_found = check_if_list_found_in_text(sentence,FIND_WORDS)\n",
        "  num_found = len(list_found)\n",
        "\n",
        "  ss = sid.polarity_scores(sentence) #NLTK\n",
        "  df = pd.DataFrame.from_dict(ss,orient = \"index\").T\n",
        "  df['transformers_score'] = dict_transformers['score'] #tranformers\n",
        "  df['transformers_label'] = dict_transformers['label']\n",
        "  df['text'] = sentence\n",
        "  df['keywords_found'] = num_found\n",
        "  return pd.concat(dict_sentiment)\n",
        "\n",
        "def filter_stopwords(sent):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  word_tokens = word_tokenize(sent)\n",
        "  filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "  filtered_sentence = []\n",
        "  for w in word_tokens:\n",
        "      if w not in stop_words:\n",
        "          filtered_sentence.append(w)\n",
        "  return ' '.join(filtered_sentence)\n",
        "\n",
        "def df_from_text(text):\n",
        "  sentence_list = tokenize.sent_tokenize(text)\n",
        "  sentence_list\n",
        "  sid = SentimentIntensityAnalyzer()\n",
        "  list_df = []\n",
        "  for sentence in sentence_list:\n",
        "      sentence = filter_stopwords(sentence)\n",
        "      list_found = check_if_list_found_in_text(sentence,FIND_WORDS)\n",
        "      num_found = len(list_found)\n",
        "#if using transformers...\n",
        "#nlp = pipeline('sentiment-analysis')\n",
        "#nlp('We are very happy to include pipeline into the transformers repository.')\n",
        "#>>> {'label': 'POSITIVE', 'score': 0.99893874}\n",
        "      ss = sid.polarity_scores(sentence)\n",
        "      df = pd.DataFrame.from_dict(ss,orient = \"index\").T\n",
        "      df['text'] = sentence\n",
        "      df['keywords_found'] = num_found\n",
        "      list_df.append(df)\n",
        "      return pd.concat(list_df)\n",
        "\n",
        "def py_write_log(str_text):\n",
        "    t = time.localtime()\n",
        "    current_time = time.strftime(\"%H:%M:%S\", t)\n",
        "    print(str_text)\n",
        "    f = open(LOGFILE, \"a\")\n",
        "    f.write(current_time+\": \"+str_text)\n",
        "    f.close()\n",
        "    return\n",
        "\n",
        "def func_sentiment(row):\n",
        "    df = df_from_text(row['text']) #neg neu pos compound text keywords_found\n",
        "    neu = df.iloc[0]['neu']\n",
        "    pos = df.iloc[0]['pos']\n",
        "    neg = df.iloc[0]['neg']\n",
        "    num_rows = 1\n",
        "    compound = df.iloc[0]['compound']\n",
        "    text = df.iloc[0]['text']\n",
        "    keywords_found = df.iloc[0]['keywords_found']\n",
        "    return pd.Series([row['ticker'],row['section'],row['type'],row['period_date'],neu,pos,neg,compound,keywords_found,text,num_rows])\n",
        "\n",
        "master_list_df = []\n",
        "list_tickers = df_tickers['Symbol']\n",
        "#list_tickers = ['MMM']\n",
        "\n",
        "for ticker in list_tickers:\n",
        "    py_write_log(\"working on...\"+ticker)\n",
        "    tic = time.perf_counter()\n",
        "\n",
        "    if path.exists(\"sec_data_folder/\"+ticker+\".csv\"):\n",
        "\n",
        "        df_text = pd.read_csv(\"sec_data_folder/\"+ticker+\".csv\")\n",
        "        if len(df_text) > 0:\n",
        "\n",
        "            df_text['ticker'] = ticker\n",
        "\n",
        "            df_discussion = df_text[df_text['section']=='discussion']\n",
        "\n",
        "            df_out = df_discussion.parallel_apply(func_sentiment, axis=1)\n",
        "            df_out.columns = ['ticker','section','type','period_date','neu','neg','pos','compound','keywords_found','text','num_rows']\n",
        "            #df_out.to_csv(\"test.csv\")\n",
        "            if len(df_out) > 0:\n",
        "\n",
        "                df_out = df_out.groupby(['ticker','period_date','type']).sum().reset_index()\n",
        "                df_out['neg'] = df_out['neg']/df_out['num_rows']\n",
        "                df_out['neu'] = df_out['neu']/df_out['num_rows']\n",
        "                df_out['pos'] = df_out['pos']/df_out['num_rows']\n",
        "                df_out['compound'] = df_out['compound']/df_out['num_rows']\n",
        "\n",
        "                df_error = df_out[df_out['compound'] == 0]\n",
        "                if not df_error.empty:\n",
        "                    py_write_log(\"zero values...\"+ticker)\n",
        "                    df_error.to_csv('sec_nlp_errors.csv',mode = 'a')\n",
        "\n",
        "                df_out.drop(['keywords_found'],axis = 1)\n",
        "                df_out['compound_baseline'] = df_out['compound'] / df_out['compound'].mean()\n",
        "                df_out['neg_baseline'] = df_out['neg'] / df_out['neg'].mean()\n",
        "                df_out['pos_baseline'] = df_out['pos'] / df_out['pos'].mean()\n",
        "                df_out['compound_bdiff'] = df_out['compound_baseline'].diff()\n",
        "                df_out['neg_bdiff'] = df_out['neg_baseline'].diff()\n",
        "                df_out['pos_bdiff'] = df_out['pos_baseline'].diff()\n",
        "                df_out['compound_zscore'] = (df_out['compound'] - df_out['compound'].mean())/df_out['compound'].std(ddof=0)\n",
        "\n",
        "                #always cache\n",
        "                str_score_file = \"sec_data_folder/\"+ticker+\"_score.csv\"\n",
        "                df_out.to_csv(str_score_file)\n",
        "\n",
        "                master_list_df.append(df_out)\n",
        "            else:\n",
        "                py_write_log(\"missing...\"+ticker)\n",
        "        else:\n",
        "            py_write_log(ticker+\" has no data.\")\n",
        "    toc = time.perf_counter()\n",
        "    py_write_log(f\"Text processed in {toc - tic:0.4f} seconds\")\n",
        "\n",
        "if master_list_df:\n",
        "    df_data = pd.concat(master_list_df)\n",
        "    df_data.to_csv('df_data.csv')\n",
        "\n",
        "df = df_data[['period_date','ticker','compound_baseline']]\n",
        "df['quarter_end'] = pd.to_datetime(df['period_date'])\n",
        "df['quarter_end'] = df.quarter_end.map(lambda x: x.strftime('%Y-%m-%d'))\n",
        "\n",
        "#modify odd quarter ends\n",
        "df.loc[df.quarter_end == '2017-04-01', 'quarter_end'] = '2017-03-31'\n",
        "df.loc[df.quarter_end == '2017-07-01', 'quarter_end'] = '2017-06-30'\n",
        "df.loc[df.quarter_end == '2018-04-01', 'quarter_end'] = '2018-03-31'\n",
        "df.loc[df.quarter_end == '2018-07-01', 'quarter_end'] = '2018-06-30'\n",
        "\n",
        "df['quarter_end'] = pd.to_datetime(df['quarter_end'])\n",
        "df['quarter_end'] = df['quarter_end'].dt.to_period('q').dt.end_time #floor at end of quarter\n",
        "df['quarter_end'] = df.quarter_end.map(lambda x: x.strftime('%Y-%m-%d')) #format\n",
        "\n",
        "import numpy as np\n",
        "df_data_pivot = pd.pivot_table(df, values='compound_baseline', index=['quarter_end'],\n",
        "                columns=['ticker'], aggfunc=np.sum, fill_value=0).reset_index()\n",
        "\n",
        "df_data_pivot.to_csv(\"df_data_pivot.csv\")\n",
        "\n",
        "print('done!')\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandarallel in /usr/local/lib/python3.6/dist-packages (1.4.8)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from pandarallel) (0.3.1.1)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "INFO: Pandarallel will run on 4 workers.\n",
            "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
            "working on...FB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Text processed in 30.6623 seconds\n",
            "working on...AMZN\n",
            "Text processed in 32.4589 seconds\n",
            "working on...PG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuyztVzxz7uo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#post pivot data to Google Sheet\n",
        "\n",
        "!pip install --upgrade -q gspread\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials as GC\n",
        "gc = gspread.authorize(GC.get_application_default())\n",
        "# create, and save df \n",
        "from gspread_dataframe import set_with_dataframe\n",
        "title = 'sec_nlp_data'\n",
        "gc.create(title)  # if not exist\n",
        "sheet = gc.open(title).sheet1\n",
        "set_with_dataframe(sheet, df_data_pivot) \n",
        "print('done!')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcrhRus4oJJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#a sample chart of one ticker.\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "df_plot = df_data[df_data['ticker'] == 'XOM'] #setting w copy warning\n",
        "\n",
        "df_plot['period_date'] = pd.to_datetime(df_plot['period_date'])\n",
        "df_plot['period_date'] = df_plot.period_date.map(lambda x: x.strftime('%Y-%m-%d')) #format\n",
        "\n",
        "df_plot.compound = pd.to_numeric(df_plot.compound)\n",
        "\n",
        "df_plot.plot.bar(x='period_date', y='compound', rot=90,title='XOM')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0MHyEDT9t7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip -r Archives.zip Archives"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}